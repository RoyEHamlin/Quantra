* Deep Learning Notes
** Introduction
*** ğŸ”˜ Course Introduction
*** ğŸ”˜ Application of RL in Trading
*** ğŸ”˜ Live Trading Overview
*** ğŸ”˜ Course Structure
*** ğŸ”˜ Course Structure Flow Diagram
*** ğŸ”˜ Quantra Features and Guidance
** Need for Reinforcement Learning
:LOGBOOK:
CLOCK: [2021-11-20 Sat 10:20]--[2021-11-20 Sat 10:36] => 0:00
:END: 
*** ğŸ”˜ Introduction to Reinforcement Learning
*** ğŸ”˜ Dilemma of Decision making
*** ğŸ”˜ Design Algorithm for Promotion
*** ğŸ”˜ Factors in Trading
*** ğŸ”˜ Impact of Decision
*** ğŸ”˜ Decision Problem and Trading
*** ğŸ”˜ Delayed Gratification
- function of time x utility
- Reinforced learning helps deal w/
*** ğŸ”˜ Reward Based on Delayed Gratification
- Certain factors to consider are...
    - higher/stronger/utility (y axis)
    - longer lasting (x axis)
*** ğŸ”˜ Reward Based on Time
*** ğŸ”˜ Designing Delayed Gratification Algorithm
- "We can only classify a decision once the reward was received"
- "Also, in classic machine learning, a right decision that maximises future reward is not labelled as beneficial."
*** ğŸ”˜ Delayed Gratification Using RL
- "RL model will look at the final reward and go backwards, assigning an implied reward at each step, till we reach the start"
** State, Actions and Rewards
*** ğŸ”˜ States, Actions and Rewards
*** ğŸ”˜ Definition of State
*** ğŸ”˜ What Can Be Added in State
*** ğŸ”˜ Need for Reward
*** ğŸ”˜ Identifying the Reward
*** ğŸ”˜ Actions in Reinforcement Learning
*** ğŸ”˜ Defining Action for Self Driving Car
*** ğŸ”˜ Limitation of Profit as Reward
*** ğŸ”˜ Reward Function Design
*** ğŸ”˜ Poorly Designed Reward Function
*** ğŸ”˜ Metrics of Reward System
*** ğŸ”˜ Next Step in Reinforcement Learning
** Q Learning
*** ğŸ”˜ Creating the Q Table
*** ğŸ”˜ Requirement of Q Table
*** ğŸ”˜ Difference between Q and R Table
*** ğŸ”˜ Representation of Q Table
*** ğŸ”˜ Identifying the Bellman Equation
*** ğŸ”˜ Importance of the Bellman Equation
*** ğŸ”˜ Solving the Bellman Equation
*** ğŸ”˜ Finding Q Table Value
*** ğŸ”˜ Zero Learning Rate Impact
*** ğŸ”˜ High Learning Rate
*** ğŸ”˜ Traditional vs Deep RL
*** ğŸ”˜ Action Based on Q Value
*** ğŸ”˜ DQN and Experience Relay
*** ğŸ”˜ I/O of NN
*** ğŸ”˜ Definition of Experience Replay
*** ğŸ”˜ Additional Reading
** State Construction
*** ğŸ”˜ State Construction
*** ğŸ”˜ Raw Price Data as Input Feature
*** ğŸ”˜ Properties of Input Features
*** ğŸ”˜ Characteristics of Input Features
*** ğŸ”˜ Returns From Price Series
*** ğŸ”˜ Technical Indicators in a State
*** ğŸ”˜ Moving Average as Input Feature
*** ğŸ”˜ Input Features
*** ğŸ”˜ Role of Information Coefficient
*** ğŸ”˜ Avoiding Correlated Inputs
*** ğŸ”˜ Time Signature as Input Feature
*** ğŸ”˜ Additional Reading
** Policies in Reinforcement Learning
*** ğŸ”˜ Policies in Reinforcement Learning
*** ğŸ”˜ Definition of Policy
*** ğŸ”˜ Types of Action
*** ğŸ”˜ Exploration Versus Exploitation
*** ğŸ”˜ Best Reinforcement Learning Policy
*** ğŸ”˜ Use of Epsilon Value
*** ğŸ”˜ Function to Calculate Epsilon
*** ğŸ”˜ Plotting Epsilon Value Curve
*** ğŸ”˜ Probability of Random Number
*** ğŸ”˜ Reduction of Exploration Rate
*** ğŸ”˜ Additional Reading
** Challenges in Reinforcement Learning
*** ğŸ”˜ Difficulties in RL
*** ğŸ”˜ Difference Between Chaos Types
*** ğŸ”˜ Importance of Type 2 Chaos
*** ğŸ”˜ Efficiency of Noise Filters
*** ğŸ”˜ Effect of Changing Market Regime
*** ğŸ”˜ Reinforcement Learning Concept
** Initialise Game Class
*** ğŸ”˜ Introduction to Part II
*** ğŸ”˜ How to Use Jupyter Notebook?
*** ğŸ”˜ Initialise Game Class
*** ğŸ”˜ Read Price Data
*** ğŸ”˜ Resample Price Data
*** ğŸ”˜ Resampling Price Bars
** Positions and Rewards
*** ğŸ”˜ Positions and Rewards
*** ğŸ”˜ Element of Reinforcement Learning
*** ğŸ”˜ What Action Do You Take?
*** ğŸ”˜ Update the Positions
*** ğŸ”˜ Same Action
*** ğŸ”˜ No Position and Buy Action
*** ğŸ”˜ Opposite Action
*** ğŸ”˜ Reward System
*** ğŸ”˜ Calculate Percentage PnL
*** ğŸ”˜ Categorical PnL Reward
*** ğŸ”˜ Difference Between Two PnL Rewards
*** ğŸ”˜ Additional Reading
** Input Features
*** ğŸ”˜ Input Features
*** ğŸ”˜ Why Time Signature?
*** ğŸ”˜ Granularity of Candlesticks
*** ğŸ”˜ Which Technical Indicators?
*** ğŸ”˜ Candlestick Input Features
*** ğŸ”˜ Why Stationary Features?
*** ğŸ”˜ Endogenous Features
*** ğŸ”˜ Exogenous Features
** Construct and Assemble State
*** ğŸ”˜ Construct and Assemble State
*** ğŸ”˜ How to Make Data Stationary?
*** ğŸ”˜ Get Last N Time Bars
*** ğŸ”˜ Size of State Vector
*** ğŸ”˜ Output of the Code
*** ğŸ”˜ Get Last N Timebars
*** ğŸ”˜ Flatten the Array
*** ğŸ”˜ Normalise Candlesticks
*** ğŸ”˜ Calculate RSI
*** ğŸ”˜ Minute Price Data and Resampling Techniques
*** ğŸ”˜ Assemble States
*** ğŸ”˜ Calculate Aroon Oscillator
*** ğŸ”˜ Datetime
*** ğŸ”˜ Time of the Day
*** ğŸ”˜ Day of the Week
*** ğŸ”˜ Additional Reading
